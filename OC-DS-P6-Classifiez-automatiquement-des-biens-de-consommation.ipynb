{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a26ed8ec",
   "metadata": {},
   "source": [
    "##### Mise en forme des dossiers de travail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bde1967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.16\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5b01ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.getcwd().split('\\\\')[-1]!='OC-DS-P6-Classifiez-automatiquement-des-biens-de-consommation':\n",
    "    # si valeur diff 'C:\\\\Users\\\\...\\\\Projets\\\\P6\\\\OC-DS-P6-Classifiez-automatiquement-des-biens-de-consommation'\n",
    "    os.chdir('OC-DS-P6-Classifiez-automatiquement-des-biens-de-consommation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c28590",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac61941d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow_core as tfc\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))#tf.config.list_physical_devices('GPU')\n",
    "print('la version de tensorflow est: ', tf.__version__)\n",
    "#print('la version de tensorflow_core est: ', tfc.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33a3f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/flipkart_com-ecommerce_sample_1050.csv')   \n",
    "\n",
    "df['cat_1'] = df.product_category_tree.str.replace('\\\\[|\\\\\"','',regex=True).str.split(\" >> \",expand = True)[0]\\\n",
    ".apply(lambda x:x.replace(' ','_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555a00fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (224,224)\n",
    "batch_size = 32\n",
    "epochs = 75\n",
    "\n",
    "train_path = 'data/cleaned/Images/train'\n",
    "test_path  = 'data/cleaned/Images/test'\n",
    "all_path   = 'data/cleaned/Images/all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b91513f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "if not os.path.exists( train_path ): \n",
    "    os.makedirs( train_path )\n",
    "    \n",
    "if not os.path.exists( test_path ): \n",
    "    os.makedirs( test_path )\n",
    "\n",
    "for repertoire in df.cat_1.unique():\n",
    "    \n",
    "    Xdf = df.loc[df.cat_1==repertoire]\n",
    "    #print(df.iloc[i][['cat_1']].values[0])\n",
    "    \n",
    "    # Create the repositorys only if it doesn't existe yet\n",
    "    for t in [train_path, test_path,all_path]:\n",
    "\n",
    "        if not os.path.exists( t+'/'+\"_\".join(repertoire.split(' ')) ): \n",
    "            os.makedirs( t+'/'+\"_\".join(repertoire.split(' ')) )\n",
    "    \n",
    "    # create train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split( Xdf[['image']], Xdf[['cat_1']], test_size=0.33, random_state=42)\n",
    "    \n",
    "    # resize and write .jpg train files\n",
    "    for img in X_train.values:  \n",
    "        image = Image.open('data/source/raw_folder_image/'+img[0]) \n",
    "\n",
    "        # Resizing image\n",
    "        new_image = image.resize((224, 224))\n",
    "\n",
    "        # Sauvegarde image format JPEG\n",
    "        new_image.save(train_path+'/'+\"_\".join(repertoire.split(' '))+'/'+img[0],format = 'JPEG')\n",
    "    \n",
    "    # resize and write .jpg test files\n",
    "    for img in X_test.values:  \n",
    "        image = Image.open('data/source/raw_folder_image/'+img[0]) \n",
    "\n",
    "        # Resizing image\n",
    "        new_image = image.resize((224, 224))\n",
    "\n",
    "        # Sauvegarde image format JPEG\n",
    "        new_image.save(test_path+'/'+\"_\".join(repertoire.split(' '))+'/'+img[0],format = 'JPEG')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2e72d3",
   "metadata": {},
   "source": [
    "## Utilisation du VGG-16 pré-entraîné"
   ]
  },
  {
   "cell_type": "raw",
   "id": "db3024c9",
   "metadata": {},
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "model = VGG16() # Création du modèle VGG-16 implementé par Keras"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1834d9b9",
   "metadata": {},
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import glob2\n",
    "import random\n",
    "\n",
    "listJPEG = glob2.glob(\"data/source/raw_folder_image/*.jpg\",recursive = True)\n",
    "listJPEG = [x.split('\\\\')[1] for x in listJPEG]\n",
    "\n",
    "img = load_img('data/source/raw_folder_image/'+random.sample(listJPEG,1)[0], target_size=(224, 224))  # Charger l'image\n",
    "img"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7860747f",
   "metadata": {},
   "source": [
    "img = img_to_array(img)  # Convertir en tableau numpy\n",
    "img = img.reshape((1, img.shape[0], img.shape[1], img.shape[2]))  # Créer la collection d'images (un seul échantillon)\n",
    "img = preprocess_input(img)  # Prétraiter l'image comme le veut VGG-16"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1d1c2d56",
   "metadata": {},
   "source": [
    "from keras.applications.vgg16 import decode_predictions\n",
    "\n",
    "y = model.predict(img)  # Prédir la classe de l'image (parmi les 1000 classes d'ImageNet)\n",
    "\n",
    "# Afficher les 3 classes les plus probables\n",
    "print('Top 3 :', decode_predictions(y, top=3)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137d87d2",
   "metadata": {},
   "source": [
    "## Transfert Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e723e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras import Model\n",
    "# Charger VGG-16 pré-entraîné sur ImageNet et sans les couches fully-connected\n",
    "model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "296c4975",
   "metadata": {},
   "source": [
    "Lors du chargement d'un modèle donné, l'argument « include_top » peut être défini sur False , auquel cas les couches de sortie entièrement connectées du modèle utilisé pour faire des prédictions ne sont pas chargées, ce qui permet d'ajouter et de former une nouvelle couche de sortie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d03f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG16(weights=\"imagenet\", input_shape=(224, 224, 3)).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32871e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1be0049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On entraîne seulement le nouveau classifieur et on ne ré-entraîne pas les autres couches :\n",
    "for layer in model.layers:\n",
    "   layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb119f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob2\n",
    "train_folders = glob2.glob(train_path+'/*')\n",
    "\n",
    "# Récupérer la sortie de ce réseau et ajout d'une couche de Flatten \n",
    "flat1 = Flatten()(model.output)\n",
    "\n",
    "class1 = Dense(1024, activation='relu')(flat1)\n",
    "class2 = Dense(1024, activation='relu')(class1)\n",
    "\n",
    "# Ajouter la nouvelle couche fully-connected pour la classification à 7 classes\n",
    "predictions = Dense(len(train_folders), activation='softmax')(class2)\n",
    "\n",
    "# Définir le nouveau modèle\n",
    "new_model = Model(inputs=model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3b8d6e81",
   "metadata": {},
   "source": [
    "import glob2\n",
    "train_folders = glob2.glob(train_path+'/*')\n",
    "\n",
    "# Récupérer la sortie de ce réseau et ajout d'une couche de Flatten \n",
    "flat1 = Flatten()(model.output)\n",
    "\n",
    "# Ajouter la nouvelle couche fully-connected pour la classification à 7 classes\n",
    "predictions = Dense(len(train_folders), activation='softmax')(flat1)\n",
    "\n",
    "# Définir le nouveau modèle\n",
    "new_model = Model(inputs=model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6edf956",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "95f8f1a6",
   "metadata": {},
   "source": [
    "# Ici, on entraîne tout le réseau, donc il faut rendre toutes les couches \"entraînables\" :\n",
    "for layer in model.layers:\n",
    "   layer.trainable = True"
   ]
  },
  {
   "cell_type": "raw",
   "id": "de7ac13b",
   "metadata": {},
   "source": [
    "# On entraîne le nouveau classifieur et les couches hautes :\n",
    "# Ne pas entraîner les 5 premières couches (les plus basses) \n",
    "for layer in model.layers[:5]:\n",
    "   layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5ea64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiler le modèle \n",
    "from keras import optimizers\n",
    "adam = optimizers.Adam()\n",
    "\n",
    "new_model.compile(loss=\"categorical_crossentropy\", \n",
    "                  optimizer= adam, #optimizers.SGD(lr=0.0001, momentum=0.9), # adam\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e14421",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "72bfb7a9",
   "metadata": {},
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "\n",
    "train_image_generator = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "test_image_generator = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89da3774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "\n",
    "train_image_generator = ImageDataGenerator(\n",
    "    rescale=1./255)\n",
    "\n",
    "test_image_generator = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d8bee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n",
    "                                                           color_mode = \"rgb\",\n",
    "                                                           directory= train_path,\n",
    "                                                           shuffle=True,\n",
    "                                                           target_size=IMAGE_SIZE, \n",
    "                                                           class_mode='categorical',\n",
    "                                                           save_to_dir='data/cleaned/Images/augmented images train/',\n",
    "                                                           save_prefix='augm_',\n",
    "                                                            save_format='jpg',\n",
    "                                                          )\n",
    "\n",
    "val_data_gen = test_image_generator.flow_from_directory(batch_size = batch_size, \n",
    "                                                              directory = test_path, \n",
    "                                                              color_mode = 'rgb',\n",
    "                                                              target_size=IMAGE_SIZE,\n",
    "                                                              class_mode='categorical')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bbea57",
   "metadata": {},
   "source": [
    "## Entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a466b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "new_model.fit_generator(train_data_gen, \n",
    "                    epochs=epochs,  # one forward/backward pass of training data\n",
    "                    steps_per_epoch=700 //batch_size,  # number of images comprising of one epoch\n",
    "                    validation_data=val_data_gen, # Or validation_data=valid_generator\n",
    "                    # validation_steps= 350 //batch_size\n",
    "                    verbose = 2\n",
    "                       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c465cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(new_model.history.history['accuracy'],label='train')\n",
    "plt.plot(new_model.history.history['val_accuracy'],label='test')\n",
    "plt.title('CNN Model accuracy values')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "53a1ac19",
   "metadata": {},
   "source": [
    "# Entraîner sur les données d'entraînement (X_train, y_train)\n",
    "model_info = new_model.fit(train_data_gen, validation_data=val_data_gen, epochs=epochs, verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
