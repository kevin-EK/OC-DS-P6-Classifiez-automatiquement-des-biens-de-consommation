{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a26ed8ec",
   "metadata": {},
   "source": [
    "##### Mise en forme des dossiers de travail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bde1967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.16\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5b01ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.getcwd().split('\\\\')[-1]!='OC-DS-P6-Classifiez-automatiquement-des-biens-de-consommation':\n",
    "    # si valeur diff 'C:\\\\Users\\\\...\\\\Projets\\\\P6\\\\OC-DS-P6-Classifiez-automatiquement-des-biens-de-consommation'\n",
    "    os.chdir('OC-DS-P6-Classifiez-automatiquement-des-biens-de-consommation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52c28590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 13610700229320335087\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.python.client.device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac61941d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "la version de tensorflow est:  2.3.0\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow_core as tfc\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))#tf.config.list_physical_devices('GPU')\n",
    "print('la version de tensorflow est: ', tf.__version__)\n",
    "#print('la version de tensorflow_core est: ', tfc.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a33a3f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/flipkart_com-ecommerce_sample_1050.csv')   \n",
    "\n",
    "df['cat_1'] = df.product_category_tree.str.replace('\\\\[|\\\\\"','',regex=True).str.split(\" >> \",expand = True)[0]\\\n",
    ".apply(lambda x:x.replace(' ','_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "555a00fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (224,224)\n",
    "batch_size = 32\n",
    "epochs = 75\n",
    "\n",
    "train_path = 'data/cleaned/Images/train'\n",
    "test_path  = 'data/cleaned/Images/test'\n",
    "all_path   = 'data/cleaned/Images/all'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "72e7abcf",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "if not os.path.exists( train_path ): \n",
    "    os.makedirs( train_path )\n",
    "    \n",
    "if not os.path.exists( test_path ): \n",
    "    os.makedirs( test_path )\n",
    "\n",
    "for repertoire in df.cat_1.unique():\n",
    "    \n",
    "    Xdf = df.loc[df.cat_1==repertoire]\n",
    "    #print(df.iloc[i][['cat_1']].values[0])\n",
    "    \n",
    "    # Create the repositorys only if it doesn't existe yet\n",
    "    for t in [train_path, test_path,all_path]:\n",
    "\n",
    "        if not os.path.exists( t+'/'+\"_\".join(repertoire.split(' ')) ): \n",
    "            os.makedirs( t+'/'+\"_\".join(repertoire.split(' ')) )\n",
    "    \n",
    "    # create train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split( Xdf[['image']], Xdf[['cat_1']], test_size=0.33, random_state=42)\n",
    "    \n",
    "    # resize and write .jpg train files\n",
    "    for img in X_train.values:  \n",
    "        image = Image.open('data/source/raw_folder_image/'+img[0]) \n",
    "\n",
    "        # Resizing image\n",
    "        new_image = image.resize((224, 224))\n",
    "\n",
    "        # Sauvegarde image format JPEG\n",
    "        new_image.save(train_path+'/'+\"_\".join(repertoire.split(' '))+'/'+img[0],format = 'JPEG')\n",
    "    \n",
    "    # resize and write .jpg test files\n",
    "    for img in X_test.values:  \n",
    "        image = Image.open('data/source/raw_folder_image/'+img[0]) \n",
    "\n",
    "        # Resizing image\n",
    "        new_image = image.resize((224, 224))\n",
    "\n",
    "        # Sauvegarde image format JPEG\n",
    "        new_image.save(test_path+'/'+\"_\".join(repertoire.split(' '))+'/'+img[0],format = 'JPEG')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2e72d3",
   "metadata": {},
   "source": [
    "## Utilisation du VGG-16 pré-entraîné"
   ]
  },
  {
   "cell_type": "raw",
   "id": "db3024c9",
   "metadata": {},
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "model = VGG16() # Création du modèle VGG-16 implementé par Keras"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1834d9b9",
   "metadata": {},
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import glob2\n",
    "import random\n",
    "\n",
    "listJPEG = glob2.glob(\"data/source/raw_folder_image/*.jpg\",recursive = True)\n",
    "listJPEG = [x.split('\\\\')[1] for x in listJPEG]\n",
    "\n",
    "img = load_img('data/source/raw_folder_image/'+random.sample(listJPEG,1)[0], target_size=(224, 224))  # Charger l'image\n",
    "img"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7860747f",
   "metadata": {},
   "source": [
    "img = img_to_array(img)  # Convertir en tableau numpy\n",
    "img = img.reshape((1, img.shape[0], img.shape[1], img.shape[2]))  # Créer la collection d'images (un seul échantillon)\n",
    "img = preprocess_input(img)  # Prétraiter l'image comme le veut VGG-16"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1d1c2d56",
   "metadata": {},
   "source": [
    "from keras.applications.vgg16 import decode_predictions\n",
    "\n",
    "y = model.predict(img)  # Prédir la classe de l'image (parmi les 1000 classes d'ImageNet)\n",
    "\n",
    "# Afficher les 3 classes les plus probables\n",
    "print('Top 3 :', decode_predictions(y, top=3)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137d87d2",
   "metadata": {},
   "source": [
    "## Transfert Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39e723e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras import Model\n",
    "# Charger VGG-16 pré-entraîné sur ImageNet et sans les couches fully-connected\n",
    "model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "296c4975",
   "metadata": {},
   "source": [
    "Lors du chargement d'un modèle donné, l'argument « include_top » peut être défini sur False , auquel cas les couches de sortie entièrement connectées du modèle utilisé pour faire des prédictions ne sont pas chargées, ce qui permet d'ajouter et de former une nouvelle couche de sortie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01d03f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "VGG16(weights=\"imagenet\", input_shape=(224, 224, 3)).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32871e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1be0049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On entraîne seulement le nouveau classifieur et on ne ré-entraîne pas les autres couches :\n",
    "for layer in model.layers:\n",
    "   layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2071551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob2\n",
    "train_folders = glob2.glob(train_path+'/*')\n",
    "\n",
    "# Récupérer la sortie de ce réseau et ajout d'une couche de Flatten \n",
    "flat1 = Flatten()(model.output)\n",
    "\n",
    "class1 = Dense(1024, activation='relu')(flat1)\n",
    "class2 = Dense(1024, activation='relu')(class1)\n",
    "\n",
    "# Ajouter la nouvelle couche fully-connected pour la classification à 7 classes\n",
    "predictions = Dense(len(train_folders), activation='softmax')(class2)\n",
    "\n",
    "# Définir le nouveau modèle\n",
    "new_model = Model(inputs=model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "73603401",
   "metadata": {},
   "source": [
    "import glob2\n",
    "train_folders = glob2.glob(train_path+'/*')\n",
    "\n",
    "# Récupérer la sortie de ce réseau et ajout d'une couche de Flatten \n",
    "flat1 = Flatten()(model.output)\n",
    "\n",
    "# Ajouter la nouvelle couche fully-connected pour la classification à 7 classes\n",
    "predictions = Dense(len(train_folders), activation='softmax')(flat1)\n",
    "\n",
    "# Définir le nouveau modèle\n",
    "new_model = Model(inputs=model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6edf956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              25691136  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 7175      \n",
      "=================================================================\n",
      "Total params: 41,462,599\n",
      "Trainable params: 26,747,911\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "95f8f1a6",
   "metadata": {},
   "source": [
    "# Ici, on entraîne tout le réseau, donc il faut rendre toutes les couches \"entraînables\" :\n",
    "for layer in model.layers:\n",
    "   layer.trainable = True"
   ]
  },
  {
   "cell_type": "raw",
   "id": "de7ac13b",
   "metadata": {},
   "source": [
    "# On entraîne le nouveau classifieur et les couches hautes :\n",
    "# Ne pas entraîner les 5 premières couches (les plus basses) \n",
    "for layer in model.layers[:5]:\n",
    "   layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d5ea64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiler le modèle \n",
    "from keras import optimizers\n",
    "adam = optimizers.Adam()\n",
    "\n",
    "new_model.compile(loss=\"categorical_crossentropy\", \n",
    "                  optimizer= adam, #optimizers.SGD(lr=0.0001, momentum=0.9), # adam\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e14421",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d6918c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "\n",
    "train_image_generator = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "test_image_generator = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03d8bee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 700 images belonging to 7 classes.\n",
      "Found 350 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n",
    "                                                           color_mode = \"rgb\",\n",
    "                                                           directory= train_path,\n",
    "                                                           #shuffle=True,\n",
    "                                                           target_size=IMAGE_SIZE, \n",
    "                                                           class_mode='categorical',\n",
    "                                                           #save_to_dir='data/cleaned/Images/augmented images train/',\n",
    "                                                           #save_prefix='augm_',\n",
    "                                                            #save_format='jpg',\n",
    "                                                          )\n",
    "\n",
    "val_data_gen = test_image_generator.flow_from_directory(batch_size = batch_size, \n",
    "                                                              directory = test_path, \n",
    "                                                              color_mode = 'rgb',\n",
    "                                                              target_size=IMAGE_SIZE,\n",
    "                                                              class_mode='categorical')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bbea57",
   "metadata": {},
   "source": [
    "## Entrainement"
   ]
  },
  {
   "cell_type": "raw",
   "id": "46bea77f",
   "metadata": {},
   "source": [
    "%%time\n",
    "with tf.device('/cpu:0'):\n",
    "    #enter code here of tf data\n",
    "    new_model.fit_generator(train_data_gen, \n",
    "                        epochs=epochs,  # one forward/backward pass of training data\n",
    "                        #steps_per_epoch=700 //batch_size,  # number of images comprising of one epoch\n",
    "                        validation_data=val_data_gen, # Or validation_data=valid_generator\n",
    "                        # validation_steps= 350 //batch_size\n",
    "                        verbose = 1\n",
    "                           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d138a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1581706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <timed exec>:8: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/75\n",
      "22/22 [==============================] - 58s 3s/step - loss: 4.8242 - accuracy: 0.2257 - val_loss: 1.2804 - val_accuracy: 0.4914\n",
      "Epoch 2/75\n",
      "19/22 [========================>.....] - ETA: 5s - loss: 1.1821 - accuracy: 0.5629"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py37_gpu\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m               \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m               instructions)\n\u001b[1;32m--> 324\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[0;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py37_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1827\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1828\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1829\u001b[1;33m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1831\u001b[0m   @deprecation.deprecated(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py37_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py37_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py37_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py37_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py37_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py37_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py37_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py37_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py37_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with tf.device('/device:GPU:0'):\n",
    "  #code here: tf data and model\n",
    "    new_model.fit_generator(train_data_gen, \n",
    "                        epochs=epochs,  # one forward/backward pass of training data\n",
    "                        #steps_per_epoch=700 //batch_size,  # number of images comprising of one epoch\n",
    "                        validation_data=val_data_gen, # Or validation_data=valid_generator\n",
    "                        # validation_steps= 350 //batch_size\n",
    "                        verbose = 1\n",
    "                           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c465cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1371fccd508>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(new_model.history.history['accuracy'],label='train')\n",
    "plt.plot(new_model.history.history['val_accuracy'],label='test')\n",
    "plt.title('CNN Model accuracy values')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1baf6d55",
   "metadata": {},
   "source": [
    "# Entraîner sur les données d'entraînement (X_train, y_train)\n",
    "model_info = new_model.fit(train_data_gen, validation_data=val_data_gen, epochs=epochs, verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
