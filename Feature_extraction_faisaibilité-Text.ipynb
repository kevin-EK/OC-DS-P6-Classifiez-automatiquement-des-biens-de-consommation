{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeea9773",
   "metadata": {},
   "source": [
    "# Sommaire :\n",
    "\n",
    "\n",
    "### <a href=\"#C1\"> **Partie 1 : Contexte et Objectifs**</a>\n",
    "\n",
    " - Contexte\n",
    " - Objectifs\n",
    "\n",
    "<b><hr></b>\n",
    "\n",
    "### <a href=\"#C2\"> **Partie 2 : Mise en place de l'espace de travail**</a>\n",
    " - <a href=\"#C21\"> Import des packages</a>\n",
    " - <a href=\"#C22\"> Repertoire de travail</a>\n",
    " - <a href=\"#C23\"> Changement du répertoire courant</a>\n",
    " - <a href=\"#C24\"> Import des datas frames</a>\n",
    "\n",
    "<b><hr></b>\n",
    "\n",
    "### <a href=\"#C3\"> **Partie 3 : Import DataFrames**</a>\n",
    " - <a href=\"#C31\"> 3.1 Display</a>\n",
    " - <a href=\"#C32\"> 3.2 Structure du dataframes</a>\n",
    "\n",
    "<b><hr></b>\n",
    " \n",
    "### <a href=\"#C4\"> **Partie 4 : Extraction Features**</a>\n",
    " - <a href=\"#C41\"> 4.1 Bags of Words</a>\n",
    "     - <a href=\"#C411\"> 4.1.1 CountVector</a>\n",
    "     - <a href=\"#C412\"> 4.1.2 TF-Idf</a>\n",
    "</br></br>\n",
    " - <a href=\"#C42\"> 4.2 Words/Sentence Embedding Classique</a>\n",
    "      - <a href=\"#C421\"> 4.2.1 Word2Vec</a>\n",
    "      - <a href=\"#C422\"> 4.2.2 Glove</a>\n",
    "      - <a href=\"#C423\"> 4.2.3 FastText</a>\n",
    "      - <a href=\"#C424\"> 4.2.4 Autres modélisations de sujets</a>\n",
    "</br></br>\n",
    " - <a href=\"#C43\"> 4.3 Words/Sentence Embedding</a>\n",
    "      - <a href=\"#C431\"> 4.2.3 BERT</a>\n",
    "      - <a href=\"#C432\"> 4.2.4 Universel Sentence Embedding</a>\n",
    "</br>\n",
    "\n",
    " \n",
    "<b><hr></b>\n",
    "\n",
    "### <a href=\"#C5\"> **Partie 5 : Conclusion**</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2401e48",
   "metadata": {},
   "source": [
    "# <a name=\"C1\">**Partie 1 : Contexte et Objectifs**</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ce540d",
   "metadata": {},
   "source": [
    "Contexte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771a79fa",
   "metadata": {},
   "source": [
    "Objectifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d3e5766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.13\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee0b79c",
   "metadata": {},
   "source": [
    "# <a name=\"C2\"><font color='blue'>**Partie 2 : Mise en place de l'espace de travail**</font></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce48057c",
   "metadata": {},
   "source": [
    "### <a name=\"C21\"><font color='blue'>2.1 Imports packages</font></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef59597",
   "metadata": {},
   "source": [
    "###### <b><font color='blue'>2.1.0 Requirements</font></b>\n",
    "- <b>Built-in</b>       : os, warnings\n",
    "- <b>Data</b>           : pandas, numpy\n",
    "- <b>Visualisations</b> : matplotlib, seaborn\n",
    "- <b>Preprocessing</b>  : sklearn, scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6463139",
   "metadata": {},
   "source": [
    "###### <b><font color='blue'>2.1.1 Imports</font></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1cff8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in\n",
    "import os, warnings \n",
    "\n",
    "# Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "#%matplotlib inline\n",
    "\n",
    "# NLP\n",
    "import nltk #/!\\ attention use nltk.download('punkt')\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import words, stopwords\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "#cluster\n",
    "from sklearn.cluster import KMeans,MiniBatchKMeans, DBSCAN\n",
    "\n",
    "#metrics\n",
    "from sklearn.metrics import (silhouette_samples,silhouette_score, adjusted_rand_score,\n",
    "                             adjusted_mutual_info_score,confusion_matrix, pair_confusion_matrix,\n",
    "                            ConfusionMatrixDisplay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09521014",
   "metadata": {},
   "source": [
    "###### <b><font color='blue'>2.1.2 Downloads and Options</font></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1183f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of physical cores in the system is 4\n",
      "The number of logical cores in the system is 8\n"
     ]
    }
   ],
   "source": [
    "# La cpu_count méthode est utilisée pour renvoyer le nombre actuel de CPU logiques dans le système.\n",
    "import psutil\n",
    "print(\"The number of physical cores in the system is %s\" % (psutil.cpu_count(logical=False),))\n",
    "print(\"The number of logical cores in the system is %s\" % (psutil.cpu_count(logical=True),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78f178a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4ad152",
   "metadata": {},
   "source": [
    "### <a name=\"C22\"><font color='blue'>2.2 Working directory</font></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87574500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git',\n",
       " '.gitignore',\n",
       " '.ipynb_checkpoints',\n",
       " 'ancien',\n",
       " 'API rapidPI.ipynb',\n",
       " 'cc.en.100.bin',\n",
       " 'cc.en.300.bin',\n",
       " 'cc.en.300.bin.gz',\n",
       " 'data',\n",
       " 'EDA.ipynb',\n",
       " 'Feature_extraction_faisaibilité-Image.ipynb',\n",
       " 'Feature_extraction_faisaibilité-Text.ipynb',\n",
       " 'Feature_extraction_faisaibilité-Transfert learning.ipynb',\n",
       " 'model1_best_weights.h5',\n",
       " 'README.md',\n",
       " \"Étude de la faisabilité d'un moteur de classification.pptx\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1807bc",
   "metadata": {},
   "source": [
    "# <a name=\"C3\"><font color='teal'>**Partie 3 : Import DataFrames**</font></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95f6cc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "try:\n",
    "    with open('data/cleaned/description_cleaned_spacy.pkl', 'rb') as f1:\n",
    "        df = pickle.load(f1)\n",
    "except:\n",
    "    df = pd.read_csv('data/cleaned/description_cleaned_spacy.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "251345a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniq_id</th>\n",
       "      <th>image</th>\n",
       "      <th>description</th>\n",
       "      <th>description_clean</th>\n",
       "      <th>cat_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55b85ea15a1536d46b7190ad6fff8ce7</td>\n",
       "      <td>55b85ea15a1536d46b7190ad6fff8ce7.jpg</td>\n",
       "      <td>Key Features of Elegance Polyester Multicolor ...</td>\n",
       "      <td>['key', 'feature', 'elegance', 'polyester', 'm...</td>\n",
       "      <td>home furnishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7b72c92c2f6c40268628ec5f14c6d590</td>\n",
       "      <td>7b72c92c2f6c40268628ec5f14c6d590.jpg</td>\n",
       "      <td>Specifications of Sathiyas Cotton Bath Towel (...</td>\n",
       "      <td>['specification', 'sathiyas', 'cotton', 'bath'...</td>\n",
       "      <td>baby care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64d5d4a258243731dc7bbb1eef49ad74</td>\n",
       "      <td>64d5d4a258243731dc7bbb1eef49ad74.jpg</td>\n",
       "      <td>Key Features of Eurospa Cotton Terry Face Towe...</td>\n",
       "      <td>['key', 'feature', 'eurospa', 'cotton', 'terry...</td>\n",
       "      <td>baby care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d4684dcdc759dd9cdf41504698d737d8</td>\n",
       "      <td>d4684dcdc759dd9cdf41504698d737d8.jpg</td>\n",
       "      <td>Key Features of SANTOSH ROYAL FASHION Cotton P...</td>\n",
       "      <td>['key', 'feature', 'santosh', 'royal', 'fashio...</td>\n",
       "      <td>home furnishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6325b6870c54cd47be6ebfbffa620ec7</td>\n",
       "      <td>6325b6870c54cd47be6ebfbffa620ec7.jpg</td>\n",
       "      <td>Key Features of Jaipur Print Cotton Floral Kin...</td>\n",
       "      <td>['key', 'feature', 'jaipur', 'print', 'cotton'...</td>\n",
       "      <td>home furnishing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            uniq_id                                 image  \\\n",
       "0  55b85ea15a1536d46b7190ad6fff8ce7  55b85ea15a1536d46b7190ad6fff8ce7.jpg   \n",
       "1  7b72c92c2f6c40268628ec5f14c6d590  7b72c92c2f6c40268628ec5f14c6d590.jpg   \n",
       "2  64d5d4a258243731dc7bbb1eef49ad74  64d5d4a258243731dc7bbb1eef49ad74.jpg   \n",
       "3  d4684dcdc759dd9cdf41504698d737d8  d4684dcdc759dd9cdf41504698d737d8.jpg   \n",
       "4  6325b6870c54cd47be6ebfbffa620ec7  6325b6870c54cd47be6ebfbffa620ec7.jpg   \n",
       "\n",
       "                                         description  \\\n",
       "0  Key Features of Elegance Polyester Multicolor ...   \n",
       "1  Specifications of Sathiyas Cotton Bath Towel (...   \n",
       "2  Key Features of Eurospa Cotton Terry Face Towe...   \n",
       "3  Key Features of SANTOSH ROYAL FASHION Cotton P...   \n",
       "4  Key Features of Jaipur Print Cotton Floral Kin...   \n",
       "\n",
       "                                   description_clean            cat_1  \n",
       "0  ['key', 'feature', 'elegance', 'polyester', 'm...  home furnishing  \n",
       "1  ['specification', 'sathiyas', 'cotton', 'bath'...        baby care  \n",
       "2  ['key', 'feature', 'eurospa', 'cotton', 'terry...        baby care  \n",
       "3  ['key', 'feature', 'santosh', 'royal', 'fashio...  home furnishing  \n",
       "4  ['key', 'feature', 'jaipur', 'print', 'cotton'...  home furnishing  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12eef4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension du dataframe importé 1050 lignes 5 colonnes:\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimension du dataframe importé {0} lignes {1} colonnes:\".format(df.shape[0],df.shape[1]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afd1a5a",
   "metadata": {},
   "source": [
    "### <a name=\"C31\"><font color='teal'>3.1 Display</font></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43d2fc63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniq_id</th>\n",
       "      <th>image</th>\n",
       "      <th>description</th>\n",
       "      <th>description_clean</th>\n",
       "      <th>cat_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55b85ea15a1536d46b7190ad6fff8ce7</td>\n",
       "      <td>55b85ea15a1536d46b7190ad6fff8ce7.jpg</td>\n",
       "      <td>Key Features of Elegance Polyester Multicolor ...</td>\n",
       "      <td>['key', 'feature', 'elegance', 'polyester', 'm...</td>\n",
       "      <td>home furnishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7b72c92c2f6c40268628ec5f14c6d590</td>\n",
       "      <td>7b72c92c2f6c40268628ec5f14c6d590.jpg</td>\n",
       "      <td>Specifications of Sathiyas Cotton Bath Towel (...</td>\n",
       "      <td>['specification', 'sathiyas', 'cotton', 'bath'...</td>\n",
       "      <td>baby care</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            uniq_id                                 image  \\\n",
       "0  55b85ea15a1536d46b7190ad6fff8ce7  55b85ea15a1536d46b7190ad6fff8ce7.jpg   \n",
       "1  7b72c92c2f6c40268628ec5f14c6d590  7b72c92c2f6c40268628ec5f14c6d590.jpg   \n",
       "\n",
       "                                         description  \\\n",
       "0  Key Features of Elegance Polyester Multicolor ...   \n",
       "1  Specifications of Sathiyas Cotton Bath Towel (...   \n",
       "\n",
       "                                   description_clean            cat_1  \n",
       "0  ['key', 'feature', 'elegance', 'polyester', 'm...  home furnishing  \n",
       "1  ['specification', 'sathiyas', 'cotton', 'bath'...        baby care  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fac88ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniq_id</th>\n",
       "      <th>image</th>\n",
       "      <th>description</th>\n",
       "      <th>description_clean</th>\n",
       "      <th>cat_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>6acca991d2353781779b866e4f96edd9</td>\n",
       "      <td>6acca991d2353781779b866e4f96edd9.jpg</td>\n",
       "      <td>Buy Home Originals Abstract, Checkered Double ...</td>\n",
       "      <td>['home', 'original', 'abstract', 'checker', 'd...</td>\n",
       "      <td>home furnishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>f39a2cce8929f5b44087d688995994e4</td>\n",
       "      <td>f39a2cce8929f5b44087d688995994e4.jpg</td>\n",
       "      <td>Buy Tiedribbons We Love Mom With Green Backgro...</td>\n",
       "      <td>['tiedribbon', 'love', 'mom', 'green', 'backgr...</td>\n",
       "      <td>home decor &amp; festive needs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              uniq_id                                 image  \\\n",
       "714  6acca991d2353781779b866e4f96edd9  6acca991d2353781779b866e4f96edd9.jpg   \n",
       "226  f39a2cce8929f5b44087d688995994e4  f39a2cce8929f5b44087d688995994e4.jpg   \n",
       "\n",
       "                                           description  \\\n",
       "714  Buy Home Originals Abstract, Checkered Double ...   \n",
       "226  Buy Tiedribbons We Love Mom With Green Backgro...   \n",
       "\n",
       "                                     description_clean  \\\n",
       "714  ['home', 'original', 'abstract', 'checker', 'd...   \n",
       "226  ['tiedribbon', 'love', 'mom', 'green', 'backgr...   \n",
       "\n",
       "                          cat_1  \n",
       "714             home furnishing  \n",
       "226  home decor & festive needs  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f105f95",
   "metadata": {},
   "source": [
    "### <a name=\"C32\"><font color='teal'>3.2 Structure du dataframe</font></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cec98ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1050 entries, 0 to 1049\n",
      "Data columns (total 5 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   uniq_id            1050 non-null   object\n",
      " 1   image              1050 non-null   object\n",
      " 2   description        1050 non-null   object\n",
      " 3   description_clean  1050 non-null   object\n",
      " 4   cat_1              1050 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 41.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31cc4947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "uniq_id              <class 'str'>\n",
       "image                <class 'str'>\n",
       "description          <class 'str'>\n",
       "description_clean    <class 'str'>\n",
       "cat_1                <class 'str'>\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyse de la classe des entité des pd.Series\n",
    "df.apply(lambda x:type(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b9ee922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable                     Type                Data/Info\n",
      "----------------------------------------------------------\n",
      "ConfusionMatrixDisplay       type                <class 'sklearn.metrics._<...>.ConfusionMatrixDisplay'>\n",
      "DBSCAN                       type                <class 'sklearn.cluster._dbscan.DBSCAN'>\n",
      "EnglishStemmer               ABCMeta             <class 'nltk.stem.snowball.EnglishStemmer'>\n",
      "Image                        module              <module 'PIL.Image' from <...>packages\\\\PIL\\\\Image.py'>\n",
      "KMeans                       type                <class 'sklearn.cluster._kmeans.KMeans'>\n",
      "MiniBatchKMeans              type                <class 'sklearn.cluster._kmeans.MiniBatchKMeans'>\n",
      "PorterStemmer                ABCMeta             <class 'nltk.stem.porter.PorterStemmer'>\n",
      "RegexpTokenizer              ABCMeta             <class 'nltk.tokenize.regexp.RegexpTokenizer'>\n",
      "WordCloud                    type                <class 'wordcloud.wordcloud.WordCloud'>\n",
      "WordNetLemmatizer            type                <class 'nltk.stem.wordnet.WordNetLemmatizer'>\n",
      "adjusted_mutual_info_score   function            <function adjusted_mutual<...>re at 0x0000012874FDBD08>\n",
      "adjusted_rand_score          function            <function adjusted_rand_s<...>re at 0x0000012874FDB510>\n",
      "confusion_matrix             function            <function confusion_matrix at 0x0000012874FC92F0>\n",
      "df                           DataFrame                                    <...>\\n[1050 rows x 5 columns]\n",
      "f1                           BufferedReader      <_io.BufferedReader name=<...>ption_cleaned_spacy.pkl'>\n",
      "nltk                         module              <module 'nltk' from 'C:\\\\<...>ages\\\\nltk\\\\__init__.py'>\n",
      "np                           module              <module 'numpy' from 'C:\\<...>ges\\\\numpy\\\\__init__.py'>\n",
      "os                           module              <module 'os' from 'C:\\\\Us<...>_tensorflow\\\\lib\\\\os.py'>\n",
      "pair_confusion_matrix        function            <function pair_confusion_<...>ix at 0x0000012874FDB2F0>\n",
      "pd                           module              <module 'pandas' from 'C:<...>es\\\\pandas\\\\__init__.py'>\n",
      "pickle                       module              <module 'pickle' from 'C:<...>sorflow\\\\lib\\\\pickle.py'>\n",
      "plt                          module              <module 'matplotlib.pyplo<...>\\\\matplotlib\\\\pyplot.py'>\n",
      "psutil                       module              <module 'psutil' from 'C:<...>es\\\\psutil\\\\__init__.py'>\n",
      "python_version               function            <function python_version at 0x000001286BBCF510>\n",
      "re                           module              <module 're' from 'C:\\\\Us<...>_tensorflow\\\\lib\\\\re.py'>\n",
      "silhouette_samples           function            <function silhouette_samp<...>es at 0x00000128750198C8>\n",
      "silhouette_score             function            <function silhouette_score at 0x00000128750196A8>\n",
      "sns                          module              <module 'seaborn' from 'C<...>s\\\\seaborn\\\\__init__.py'>\n",
      "spacy                        module              <module 'spacy' from 'C:\\<...>ges\\\\spacy\\\\__init__.py'>\n",
      "stopwords                    LazyCorpusLoader    <WordListCorpusReader in <...>pwords' (not loaded yet)>\n",
      "warnings                     module              <module 'warnings' from '<...>rflow\\\\lib\\\\warnings.py'>\n",
      "word_tokenize                function            <function word_tokenize at 0x000001287475EAE8>\n",
      "wordpunct_tokenize           method              <bound method RegexpToken<...>E|DOTALL|MULTILINE: 56>)>\n",
      "words                        LazyCorpusLoader    <WordListCorpusReader in <...>/words' (not loaded yet)>\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1719594b",
   "metadata": {},
   "source": [
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b235f839",
   "metadata": {},
   "source": [
    "</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "198b1d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Buy Epresent Mfan 1 Fan USB USB Fan for Rs.219 online. Epresent Mfan 1 Fan USB USB Fan at best prices with FREE shipping & cash on delivery. Only Genuine Products. 30 Day Replacement Guarantee.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.description[843]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26c153e",
   "metadata": {},
   "source": [
    "# <a name=\"C4\"><font color='green'>**Partie 4 : Extraction Features**</font></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e77801b",
   "metadata": {},
   "source": [
    "## Fonction utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f867743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(list_of_docs, model):\n",
    "    \"\"\"Generate vectors for list of documents using a Word Embedding\n",
    "\n",
    "    Args:\n",
    "        list_of_docs: List of documents\n",
    "        model: Gensim's Word Embedding\n",
    "\n",
    "    Returns:\n",
    "        List of document vectors\n",
    "    \"\"\"\n",
    "    features = []\n",
    "\n",
    "    for tokens in list_of_docs:\n",
    "        zero_vector = np.zeros(model.vector_size)\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            if token in model:\n",
    "                try:\n",
    "                    vectors.append(model[token])\n",
    "                except KeyError:\n",
    "                    continue\n",
    "        if vectors:\n",
    "            vectors = np.asarray(vectors)\n",
    "            avg_vec = vectors.mean(axis=0)\n",
    "            features.append(avg_vec)\n",
    "        else:\n",
    "            features.append(zero_vector)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d6a5e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mbkmeans_clusters(\n",
    "    X, \n",
    "    k, \n",
    "    # mb, \n",
    "    print_silhouette_values, \n",
    "):\n",
    "    \"\"\"Generate clusters and print Silhouette metrics using MBKmeans\n",
    "\n",
    "    Args:\n",
    "        X: Matrix of features.\n",
    "        k: Number of clusters.\n",
    "        mb: Size of mini-batches.\n",
    "        print_silhouette_values: Print silhouette values per cluster.\n",
    "\n",
    "    Returns:\n",
    "        Trained clustering model and labels based on X.\n",
    "    \"\"\"\n",
    "    km = KMeans(n_clusters=k, init = 'k-means++',n_init=10).fit(X)\n",
    "    print(f\"For n_clusters = {k}\")\n",
    "    print(f\"Silhouette coefficient: {silhouette_score(X, km.labels_):0.2f}\")\n",
    "    print(f\"Inertia:{km.inertia_}\")\n",
    "\n",
    "    if print_silhouette_values:\n",
    "        sample_silhouette_values = silhouette_samples(X, km.labels_)\n",
    "        print(f\"Silhouette values:\")\n",
    "        silhouette_values = []\n",
    "        for i in range(k):\n",
    "            cluster_silhouette_values = sample_silhouette_values[km.labels_ == i]\n",
    "            silhouette_values.append(\n",
    "                (\n",
    "                    i,\n",
    "                    cluster_silhouette_values.shape[0],\n",
    "                    cluster_silhouette_values.mean(),\n",
    "                    cluster_silhouette_values.min(),\n",
    "                    cluster_silhouette_values.max(),\n",
    "                )\n",
    "            )\n",
    "        silhouette_values = sorted(\n",
    "            silhouette_values, key=lambda tup: tup[2], reverse=True\n",
    "        )\n",
    "        for s in silhouette_values:\n",
    "            print(\n",
    "                f\"    Cluster {s[0]}: Size:{s[1]} | Avg:{s[2]:.2f} | Min:{s[3]:.2f} | Max: {s[4]:.2f}\"\n",
    "            )\n",
    "    return km, km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d214b766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ARI_fct_raw(x_label_clust,x_label_true):\n",
    "    ARI = np.round(adjusted_rand_score(x_label_clust, x_label_true),4)\n",
    "    print(\"ARI : \", ARI)\n",
    "    return(ARI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0d7f298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn import cluster, metrics\n",
    "from sklearn import manifold, decomposition\n",
    "\n",
    "# Calcul Tsne, détermination des clusters et calcul ARI entre vrais catégorie et n° de clusters\n",
    "def ARI_fct(features) :\n",
    "    time1 = time.time()\n",
    "    num_labels=len(l_cat)\n",
    "    tsne = manifold.TSNE(n_components=2, perplexity=30, n_iter=2000, \n",
    "                                 init='random', learning_rate=200, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(features)\n",
    "    \n",
    "    # Détermination des clusters à partir des données après Tsne \n",
    "    cls = cluster.KMeans(n_clusters=num_labels, n_init=100, random_state=42)\n",
    "    cls.fit(X_tsne)\n",
    "    ARI = np.round(metrics.adjusted_rand_score(y_cat_num, cls.labels_),4)\n",
    "    time2 = np.round(time.time() - time1,0)\n",
    "    print(\"ARI : \", ARI, \"time : \", time2)\n",
    "    \n",
    "    return ARI, X_tsne, cls.labels_\n",
    "\n",
    "\n",
    "# visualisation du Tsne selon les vraies catégories et selon les clusters\n",
    "def TSNE_visu_fct(X_tsne, y_cat_num, labels, ARI) :\n",
    "    fig = plt.figure(figsize=(20,12))\n",
    "    \n",
    "    ax = fig.add_subplot(121)\n",
    "    scatter = ax.scatter(X_tsne[:,0],X_tsne[:,1], c=y_cat_num, cmap='Set1')\n",
    "    ax.legend(handles=scatter.legend_elements()[0], labels=l_cat, loc=\"best\", title=\"Categorie\")\n",
    "    plt.title('Représentation des produits par catégories réelles')\n",
    "    \n",
    "    ax = fig.add_subplot(122)\n",
    "    scatter = ax.scatter(X_tsne[:,0],X_tsne[:,1], c=labels, cmap='Set1')\n",
    "    ax.legend(handles=scatter.legend_elements()[0], labels=set(labels), loc=\"best\", title=\"Clusters\")\n",
    "    plt.title('Représentation des produits par clusters')\n",
    "    \n",
    "    plt.show()\n",
    "    print(\"ARI : \", ARI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f99151a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix( y_true, y_pred, class_labels=None, display_labels=None, sample_weight=None,\n",
    "                            normalize_f='true',        # --> option {'all', 'count', 'percent', None}\n",
    "                            cmap='Blues',\n",
    "                            ax=None,\n",
    "                            title=None,\n",
    "                            show_values='all',      # --> option {'all', 'true', 'pred'}\n",
    "                            show_colorbar=True,     # --> option {True, False}\n",
    "                            show_subtotals=True):   # --> option {True, False}\n",
    "    \n",
    "    fig = plt.figure(figsize=(15,6))\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    \n",
    "    disp = ConfusionMatrixDisplay.from_predictions(y_true, y_pred,\n",
    "                                                   xticks_rotation='vertical',\n",
    "                                                   normalize=normalize_f, \n",
    "                                                   display_labels=class_labels,cmap=plt.cm.Blues,\n",
    "                                                  ax=ax1)\n",
    "    disp.ax_.set_title('Confusion matrix, without normalization')\n",
    "    disp.ax_.set_xticklabels(range(7),rotation=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3594bcb",
   "metadata": {},
   "source": [
    "### Parametres utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd68ab8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "catégories :  ['computers', 'home furnishing', 'home decor & festive needs', 'beauty and personal care', 'baby care', 'watches', 'kitchen & dining']\n"
     ]
    }
   ],
   "source": [
    "l_cat = list(set(df['cat_1']))\n",
    "print(\"catégories : \", l_cat)\n",
    "y_cat_num = [(l_cat.index(df.iloc[i]['cat_1'])) for i in range(len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a09914ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 6}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(y_cat_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eac1899",
   "metadata": {},
   "source": [
    "### <a name=\"C41\"><font color='green'>4.1 Bag Of Words </font></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21aa549",
   "metadata": {},
   "source": [
    "### - <a name=\"C411\"><font color='green'>4.1.a Comptage de mots 1-gram </font></a> "
   ]
  },
  {
   "cell_type": "raw",
   "id": "9d7f6cf0",
   "metadata": {},
   "source": [
    "# Telecharger les modèles pré-entraînés en anglais et français\n",
    "!python -m spacy download fr_core_news_md\n",
    "!python -m spacy download fr_core_news_sm\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cbdcba19",
   "metadata": {},
   "source": [
    "# Obtenir la liste des modèles pré-entraînés disponible\n",
    "!python -m spacy validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4ea8a6c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CountVectorizer' object has no attribute 'get_feature_names_out'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-5e19c9b00680>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mvecX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[0mCountWord\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mvecX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names_out\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Notre dataset est composé de {} lignes et {} colonnes\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCountWord\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mCountWord\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CountVectorizer' object has no attribute 'get_feature_names_out'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#class LemmaTokenizer(object):\n",
    "#    def __init__(self):\n",
    "#        self.wnl = WordNetLemmatizer()\n",
    "#    def __call__(self, articles):\n",
    "#        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]\n",
    "\n",
    "def LemmaTokenizer(articles):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    return [wnl.lemmatize(t.lower()) for t in word_tokenize(articles)]\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "vectorizer = CountVectorizer(input='content', # the input is expected to be a sequence of items that can be of type string or byte\n",
    "                             encoding='utf-8', \n",
    "                             decode_error='replace', #Instruction sur ce qu'il faut faire si une séquence d'octets est donnée à analyser qui contenant \n",
    "                             #des caractères n'appartenant pas à la donnée \"encoding\"\n",
    "                             strip_accents='unicode', # Remove accents and perform other character normalization during the preprocessing step.\n",
    "                             lowercase=True, # Convert all characters to lowercase before tokenizing\n",
    "                             preprocessor=None, \n",
    "                             tokenizer=None, \n",
    "                             #stop_words='english',            \n",
    "                             token_pattern = r\"\\b[a-zA-Z]{3,}\\b\", #ascii only alpha\n",
    "                             #token_pattern = r\"(?u)\\b[a-zA-Z]+\\b\", #ascii only alpha\n",
    "                             ngram_range=(1, 1), \n",
    "                             analyzer='word', \n",
    "                             max_df=351, \n",
    "                             min_df=3, #0.0013, # < 1.5/1050\n",
    "                             max_features=None,\n",
    "                             vocabulary=None, \n",
    "                             binary=False )\n",
    "\n",
    "vecX = vectorizer.fit_transform( df.description.apply(lambda x: \" \".join([token.lemma_ for token in nlp(x.lower())]) ) )\n",
    "\n",
    "CountWord = pd.DataFrame( vecX.toarray(), columns=vectorizer.get_feature_names_out() )\n",
    "\n",
    "print(\"Notre dataset est composé de {} lignes et {} colonnes\".format(CountWord.shape[0],CountWord.shape[1]) )\n",
    "CountWord.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ef2286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utiliser un minibatch kmeans pas utils si donnée compte moins de 2000 echantillons\n",
    "clusteringCountVec, CountVeccluster_labels = mbkmeans_clusters(\n",
    "    X=CountWord,\n",
    "    k=7,\n",
    "    # mb=1024, # puissance de 2\n",
    "    print_silhouette_values=True,\n",
    ")\n",
    "print('\\n')\n",
    "print('Score ARI sur Avant diminution de dimension')\n",
    "_ = ARI_fct_raw(CountVeccluster_labels,df.cat_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ae180c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Most representative terms per cluster (based on centroids):\")\n",
    "for i in range(7):\n",
    "    tokens_per_cluster = \"\"\n",
    "    most_representative = CountWord.columns[np.argsort( clusteringCountVec.cluster_centers_[i] )[:5]]\n",
    "    for t in most_representative:\n",
    "        tokens_per_cluster += f\"{t} \"\n",
    "    print(f\"Cluster {i}: {tokens_per_cluster}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f37689",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( 'Score ARI après TSNE 2D + Clustering')\n",
    "ARI, X_tsne, labels = ARI_fct(CountWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514e94c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TSNE_visu_fct(X_tsne, y_cat_num, labels, ARI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94523e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_cat_num, labels,class_labels=l_cat,normalize_f=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca558e6",
   "metadata": {},
   "source": [
    "</Br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fddb2da",
   "metadata": {},
   "source": [
    "### - <a name=\"C412\"><font color='green'>4.1.b TF IDF </font></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35d7332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "    input='content', # the input is expected to be a sequence of items that can be of type string or byte\n",
    "    encoding='utf-8', \n",
    "    decode_error='replace', \n",
    "    strip_accents='ascii', # Remove accents and perform other character normalization during the preprocessing step.\n",
    "    lowercase=True, # Convert all characters to lowercase before tokenizing\n",
    "    preprocessor=None, \n",
    "    stop_words=\"english\", \n",
    "    token_pattern = r\"\\b[a-zA-Z]{3,}\\b\", #ascii only alpha\n",
    "    #tokenizer = RegexpTokenizer(r\"[a-zA-Z]{3,}\").tokenize,\n",
    "    tokenizer=None,\n",
    "    ngram_range=(1, 1), \n",
    "    analyzer='word', \n",
    "    max_df=351, \n",
    "    min_df=3, # < 1.5/1050\n",
    "    max_features=None,\n",
    "    vocabulary=None, binary=False ,\n",
    "    smooth_idf = True\n",
    "    )\n",
    "tfidf_values = tfidf.fit_transform( df.description.apply(lambda x: \" \".join([token.lemma_ for token in nlp(x.lower())]) ) )\n",
    "\n",
    "TFIDF_df = pd.DataFrame(tfidf_values.toarray(),columns=tfidf.get_feature_names_out())\n",
    "\n",
    "TFIDF_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ca0974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utiliser un minibatch kmeans pas utils si donnée compte moins de 2000 echantillons\n",
    "clusteringTFIDF, TFIDFcluster_labels = mbkmeans_clusters(\n",
    "    X=TFIDF_df,\n",
    "    k=7,\n",
    "    # mb=1024, # puissance de 2\n",
    "    print_silhouette_values=True,\n",
    ")\n",
    "print('\\n')\n",
    "print('Score ARI sur Avant diminution de dimension')\n",
    "_ = ARI_fct_raw(TFIDFcluster_labels,df.cat_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2e1670",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Most representative terms per cluster (based on centroids):\")\n",
    "for i in range(7):\n",
    "    tokens_per_cluster = \"\"\n",
    "    most_representative = CountWord.columns[np.argsort( clusteringTFIDF.cluster_centers_[i] )[:5]]\n",
    "    for t in most_representative:\n",
    "        tokens_per_cluster += f\"{t} \"\n",
    "    print(f\"Cluster {i}: {tokens_per_cluster}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a4ac64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( 'Score ARI après TSNE 2D + Clustering')\n",
    "ARI, X_tsne, labels = ARI_fct(TFIDF_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d3cdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "TSNE_visu_fct(X_tsne, y_cat_num, labels, ARI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582555d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_cat_num, labels,class_labels=l_cat,normalize_f=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902dac0b",
   "metadata": {},
   "source": [
    "</Br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d0e605",
   "metadata": {},
   "source": [
    "### <a name=\"C42\"><font color='green'>4.2 Word/Sentence embedding </font></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14879d2e",
   "metadata": {},
   "source": [
    "### - <a name=\"C421\"><font color='green'>4.2.a Word2Vec </font></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6768921a",
   "metadata": {},
   "source": [
    "### <font color='green'> Model entrainé localement </font>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dba63be2",
   "metadata": {},
   "source": [
    "Cette fonction possède cinq paramètres principaux :\n",
    "\n",
    "- vector_size : La dimension du vecteur créé, idéalement inférieur au nombre de mots du vocabulaire\n",
    "- window : La distance maximale entre un mot cible et les mots autour du mot cible. La fenêtre par défaut est de 5.\n",
    "- min_count : Le nombre minimum de mots à prendre en compte lors de l’apprentissage du modèle ; les mots dont l’occurrence               est inférieure à ce nombre seront ignorés. La valeur par défaut de min_count est 5.\n",
    "- worker : Le nombre de lots créés pour l’entraînement, par défaut il y en a 3."
   ]
  },
  {
   "cell_type": "raw",
   "id": "96e2432f",
   "metadata": {},
   "source": [
    "https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4\n",
    "\n",
    "La librairie Gensim est très intéressante car elle nous permet de visualiser cet embedding.\n",
    "C’est-à-dire qu’on va pouvoir analyser l’embedding en regardant quel mot est similaire à quel autre par exemple.\n",
    "Pour cet embedding, il faut que nos données soient sous forme de tokens (chaque mot séparé) et non sous forme de phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651c2a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "print(\"Entrainement du model Word2Vec...\")\n",
    "model_W2V = Word2Vec(sentences=df.description_clean, vector_size =100, window=5, min_count=1, workers=4)\n",
    "model_W2V.train(df.description_clean, total_examples=len(df.description_clean), epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07450b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca27b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_docs = vectorize(df.description_clean, model=model_W2V.wv)\n",
    "\n",
    "# utiliser un minibatch kmeans pas utils si donnée compte moins de 2000 echantillons\n",
    "clusteringW2V, W2Vcluster_labels = mbkmeans_clusters(\n",
    "    X=vectorized_docs,\n",
    "    k=7,\n",
    "    # mb=1024, # puissance de 2\n",
    "    print_silhouette_values=True,\n",
    ")\n",
    "print('\\n')\n",
    "print('Score ARI sur Avant diminution de dimension')\n",
    "_ = ARI_fct_raw(W2Vcluster_labels,df.cat_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf3381d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Most representative terms per cluster (based on centroids):\")\n",
    "for i in range(7):\n",
    "    tokens_per_cluster = \"\"\n",
    "    most_representative = model_W2V.wv.most_similar(positive=[clusteringW2V.cluster_centers_[i]], topn=10)\n",
    "    for t in most_representative:\n",
    "        tokens_per_cluster += f\"{t[0]} \"\n",
    "    print(f\"Cluster {i}: {tokens_per_cluster}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d671a174",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(TFIDF_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae7292c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( 'Score ARI après TSNE 2D + Clustering')\n",
    "ARI, X_tsne, labels = ARI_fct(pd.DataFrame(vectorized_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd3e376",
   "metadata": {},
   "outputs": [],
   "source": [
    "TSNE_visu_fct(X_tsne, y_cat_num, labels, ARI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7b7119",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_cat_num, labels,class_labels=l_cat,normalize_f=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006f85e7",
   "metadata": {},
   "source": [
    "</Br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671b5ec2",
   "metadata": {},
   "source": [
    "### <font color='green'> Model W2V pré-entrainé de google </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f59647",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from gensim.models import KeyedVectors\n",
    "modelpw2wgoogle = KeyedVectors.load_word2vec_format('data/support/Word2Vec/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545dbb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_docs = vectorize(df.description_clean, model=modelpw2wgoogle)\n",
    "\n",
    "# utiliser un minibatch kmeans pas utils si donnée compte moins de 2000 echantillons\n",
    "clusteringW2Vpre, W2Vprecluster_labels = mbkmeans_clusters(\n",
    "    X=vectorized_docs,\n",
    "    k=7,\n",
    "    # mb=1024, # puissance de 2\n",
    "    print_silhouette_values=True,\n",
    ")\n",
    "print('\\n')\n",
    "print('Score ARI sur Avant diminution de dimension')\n",
    "_ = ARI_fct_raw(W2Vprecluster_labels,df.cat_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26496c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( 'Score ARI après TSNE 2D + Clustering')\n",
    "ARI, X_tsne, labels = ARI_fct(pd.DataFrame(vectorized_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064986e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TSNE_visu_fct(X_tsne, y_cat_num, labels, ARI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d069cbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "del modelpw2wgoogle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a93a12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_cat_num, labels,class_labels=l_cat,normalize_f=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f33a29",
   "metadata": {},
   "source": [
    "</Br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d0b7cd",
   "metadata": {},
   "source": [
    "### - <a name=\"C422\"><font color='green'>4.2.b GloVe </font></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57517dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Indexing word vectors.')\n",
    "\n",
    "emmbed_dict = {}\n",
    "f = open('data/support/glove.6B/glove.6B.200d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    emmbed_dict[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(emmbed_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce48908",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "def find_similar_word(emmbedes):\n",
    "    nearest = sorted(emmbed_dict.keys(), key=lambda word: spatial.distance.euclidean(emmbed_dict[word], emmbedes))\n",
    "    return nearest\n",
    "\n",
    "find_similar_word(emmbed_dict['river'])[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7a74f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_embeddings(embedding):\n",
    "    return sorted(emmbed_dict.keys(), key=lambda word: spatial.distance.euclidean(emmbed_dict[word], embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025bd9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(find_closest_embeddings(emmbed_dict[\"king\"])[1:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b90caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(find_closest_embeddings(\n",
    "    emmbed_dict[\"king\"] - emmbed_dict[\"man\"] + emmbed_dict[\"woman\"]\n",
    ")[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a33d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversion ds vecteurs GloVe au format texte au format texte word2vec :\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove2word2vec(glove_input_file=\"data/support/glove.6B/glove.6B.300d.txt\", \n",
    "               word2vec_output_file=\"data/support/glove.6B/gensim_glove_vectors.txt\")\n",
    "\n",
    "# Chargement du model glove\n",
    "from gensim.models import KeyedVectors\n",
    "glove_model = KeyedVectors.load_word2vec_format('data/support/glove.6B/gensim_glove_vectors.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f50d38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_docs = vectorize(df.description_clean, model=glove_model)\n",
    "\n",
    "# utiliser un minibatch kmeans pas utils si donnée compte moins de 2000 echantillons\n",
    "clusteringGlove, Glovecluster_labels = mbkmeans_clusters(\n",
    "    X=vectorized_docs,\n",
    "    k=7,\n",
    "    # mb=1024, # puissance de 2\n",
    "    print_silhouette_values=True,\n",
    ")\n",
    "print('\\n')\n",
    "print('Score ARI sur Avant diminution de dimension')\n",
    "_ = ARI_fct_raw(Glovecluster_labels,df.cat_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e597ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( 'Score ARI après TSNE 2D + Clustering')\n",
    "ARI, X_tsne, labels = ARI_fct(pd.DataFrame(vectorized_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9b2253",
   "metadata": {},
   "outputs": [],
   "source": [
    "TSNE_visu_fct(X_tsne, y_cat_num, labels, ARI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ca6477",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_cat_num, labels,class_labels=l_cat,normalize_f=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8a38c3",
   "metadata": {},
   "source": [
    "</Br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ff628e",
   "metadata": {},
   "source": [
    "### - <a name=\"C423\"><font color='green'>4.2.c FastText </font></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823c0863",
   "metadata": {},
   "source": [
    "#####  <font color='green'>4.2.c.1 FastText prélearning model </font>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8ced3d8b",
   "metadata": {},
   "source": [
    "# Telecharger le model pré-entrainé\n",
    "import fasttext.util\n",
    "fasttext.util.download_model('en', if_exists='ignore')  # English"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a9d23a7b",
   "metadata": {},
   "source": [
    "import fasttext\n",
    "ft = fasttext.load_model('cc.en.300.bin')\n",
    "fasttext.util.reduce_model(ft, 100)\n",
    "ft.save_model('cc.en.100.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff98317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "# Load pre-trained FastText model\n",
    "model_path = 'cc.en.100.bin'\n",
    "model = gensim.models.fasttext.load_facebook_vectors(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf48965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Document Vectors list from Word Embedding\n",
    "vectorized_docs = vectorize(df.description_clean, model=model)\n",
    "len(vectorized_docs), len(vectorized_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa457d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_docs = vectorize(df.description_clean, model=model)\n",
    "\n",
    "# utiliser un minibatch kmeans pas utils si donnée compte moins de 2000 echantillons\n",
    "clusteringFastText, FastTextcluster_labels = mbkmeans_clusters(\n",
    "    X=vectorized_docs,\n",
    "    k=7,\n",
    "    # mb=1024, # puissance de 2\n",
    "    print_silhouette_values=True,\n",
    ")\n",
    "print('\\n')\n",
    "print('Score ARI sur Avant diminution de dimension')\n",
    "_ = ARI_fct_raw(FastTextcluster_labels,df.cat_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c1f21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( 'Score ARI après TSNE 2D + Clustering')\n",
    "ARI, X_tsne, labels = ARI_fct(pd.DataFrame(vectorized_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f59d3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TSNE_visu_fct(X_tsne, y_cat_num, labels, ARI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bff1134",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44ba6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_cat_num, labels,class_labels=l_cat,normalize_f=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f69694d",
   "metadata": {},
   "source": [
    "#####  <font color='green'>4.2.c.2 FastText local train model </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed1b6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.description_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084f59cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "model = FastText(vector_size=100, window=3)\n",
    "model.build_vocab(corpus_iterable=df.description_clean)  # scan over corpus to build the vocabulary\n",
    "\n",
    "total_words = model.corpus_total_words  # number of words in the corpus\n",
    "model.train(corpus_iterable=df.description_clean, total_words=total_words, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59320be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(FastText.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc5eb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_docs = vectorize(df.description_clean, model=model.wv)\n",
    "\n",
    "# utiliser un minibatch kmeans pas utils si donnée compte moins de 2000 echantillons\n",
    "clusteringFastText, FastTextcluster_labels = mbkmeans_clusters(\n",
    "    X=vectorized_docs,\n",
    "    k=7,\n",
    "    # mb=1024, # puissance de 2\n",
    "    print_silhouette_values=True,\n",
    ")\n",
    "print('\\n')\n",
    "print('Score ARI sur Avant diminution de dimension')\n",
    "_ = ARI_fct_raw(FastTextcluster_labels,df.cat_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c5abbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( 'Score ARI après TSNE 2D + Clustering')\n",
    "ARI, X_tsne, labels = ARI_fct(pd.DataFrame(vectorized_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd4e997",
   "metadata": {},
   "outputs": [],
   "source": [
    "TSNE_visu_fct(X_tsne, y_cat_num, labels, ARI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be83be07",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_cat_num, labels,class_labels=l_cat,normalize_f=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf884b3",
   "metadata": {},
   "source": [
    "</Br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9446c26e",
   "metadata": {},
   "source": [
    "### <a name=\"C424\"><font color='green'>4.2.d Modélisez des sujets avec des méthodes non supervisées </font></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b4c032",
   "metadata": {},
   "source": [
    "#### <font color='green'> Latent Dirichlet Allocation (LDA) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713efbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "n_topics = 100\n",
    "\n",
    "# Créer le modèle LDA\n",
    "lda = LatentDirichletAllocation(\n",
    "        n_components=n_topics, \n",
    "        max_iter=5, \n",
    "        learning_method='online', \n",
    "        learning_offset=50.,\n",
    "        random_state=0)\n",
    "\n",
    "# Fitter sur les données\n",
    "lda.fit(CountWord)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "acd2dea4",
   "metadata": {},
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic {}:\".format(topic_idx))\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 10\n",
    "display_topics(lda, CountWord.columns, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8e06bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( 'Score ARI après TSNE 2D + Clustering')\n",
    "ARI, X_tsne, labels = ARI_fct(pd.DataFrame(lda.transform(CountWord)))\n",
    "TSNE_visu_fct(X_tsne, y_cat_num, labels, ARI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5521963c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_cat_num, labels,class_labels=l_cat,normalize_f=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abdfe0f",
   "metadata": {},
   "source": [
    "#### <font color='green'> NMF (Negative Matrix Factorisation) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24832220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# NMF is able to use tf-idf\n",
    "no_topics = 100\n",
    "\n",
    "# Run NMF\n",
    "nmf = NMF(n_components=no_topics, random_state=1,  l1_ratio=.5, init='nndsvd')\n",
    "nmf.fit(tfidf_values)\n",
    "\n",
    "no_top_words = 10\n",
    "#display_topics(nmf, TFIDF_df.columns, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beb327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( 'Score ARI après TSNE 2D + Clustering')\n",
    "ARI, X_tsne, labels = ARI_fct(pd.DataFrame(nmf.transform(TFIDF_df)))\n",
    "TSNE_visu_fct(X_tsne, y_cat_num, labels, ARI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c0ba83",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_cat_num, labels,class_labels=l_cat,normalize_f=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09617cff",
   "metadata": {},
   "source": [
    "### <a name=\"C43\"><font color='green'>4.3 Effectuez des plongements de mots (word embeddings) </font></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b36184",
   "metadata": {},
   "source": [
    "### Fonctions communes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628864e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_fct(sentence) :\n",
    "    # print(sentence)\n",
    "    sentence_clean = sentence.replace('-', ' ').replace('+', ' ').replace('/', ' ').replace('#', ' ')\n",
    "    word_tokens = word_tokenize(sentence_clean)\n",
    "    return word_tokens\n",
    "\n",
    "# Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_w = list(set(stopwords.words('english'))) + ['[', ']', ',', '.', ':', '?', '(', ')']\n",
    "\n",
    "def stop_word_filter_fct(list_words) :\n",
    "    filtered_w = [w for w in list_words if not w in stop_w]\n",
    "    filtered_w2 = [w for w in filtered_w if len(w) > 2]\n",
    "    return filtered_w2\n",
    "\n",
    "# lower case et alpha\n",
    "def lower_start_fct(list_words) :\n",
    "    lw = [w.lower() for w in list_words if (not w.startswith(\"@\")) \n",
    "    #                                   and (not w.startswith(\"#\"))\n",
    "                                       and (not w.startswith(\"http\"))]\n",
    "    return lw\n",
    "\n",
    "# Lemmatizer (base d'un mot)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemma_fct(list_words) :\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lem_w = [lemmatizer.lemmatize(w) for w in list_words]\n",
    "    return lem_w\n",
    "\n",
    "# Fonction de préparation du texte pour le bag of words avec lemmatization\n",
    "def transform_bow_lem_fct(desc_text) :\n",
    "    word_tokens = tokenizer_fct(desc_text)\n",
    "    sw = stop_word_filter_fct(word_tokens)\n",
    "    lw = lower_start_fct(sw)\n",
    "    lem_w = lemma_fct(lw)    \n",
    "    transf_desc_text = ' '.join(lem_w)\n",
    "    return transf_desc_text\n",
    "\n",
    "# Fonction de préparation du texte pour le Deep learning (USE et BERT)\n",
    "def transform_dl_fct(desc_text) :\n",
    "    word_tokens = tokenizer_fct(desc_text)\n",
    "#    sw = stop_word_filter_fct(word_tokens)\n",
    "    lw = lower_start_fct(word_tokens)\n",
    "    # lem_w = lemma_fct(lw)    \n",
    "    transf_desc_text = ' '.join(lw)\n",
    "    return transf_desc_text\n",
    "\n",
    "\n",
    "df['sentence_bow_lem'] = df['description'].apply(lambda x : transform_bow_lem_fct(x))\n",
    "df['sentence_dl'] = df['description'].apply(lambda x : transform_dl_fct(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e42cf51",
   "metadata": {},
   "source": [
    "</Br>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "07deca31",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import metrics as kmetrics\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b27234",
   "metadata": {},
   "source": [
    "</Br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15214e8",
   "metadata": {},
   "source": [
    "</Br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebed34c",
   "metadata": {},
   "source": [
    "#### <a name=\"C431\"><font color='green'>4.3.a BERT </font></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4607494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# import tensorflow_hub as hub\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import metrics as kmetrics\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Bert\n",
    "import os\n",
    "import transformers\n",
    "#from transformers import *\n",
    "\n",
    "os.environ[\"TF_KERAS\"]='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671c28e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)\n",
    "print(tensorflow.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33039549",
   "metadata": {},
   "source": [
    "### Fonctions communes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e85f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de préparation des sentences\n",
    "def bert_inp_fct(sentences, bert_tokenizer, max_length) :\n",
    "    input_ids=[]\n",
    "    token_type_ids = []\n",
    "    attention_mask=[]\n",
    "    bert_inp_tot = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        bert_inp = bert_tokenizer.encode_plus(sent,\n",
    "                                              add_special_tokens = True,\n",
    "                                              max_length = max_length,\n",
    "                                              padding='max_length',\n",
    "                                              return_attention_mask = True, \n",
    "                                              return_token_type_ids=True,\n",
    "                                              truncation=True,\n",
    "                                              return_tensors=\"tf\")\n",
    "    \n",
    "        input_ids.append(bert_inp['input_ids'][0])\n",
    "        token_type_ids.append(bert_inp['token_type_ids'][0])\n",
    "        attention_mask.append(bert_inp['attention_mask'][0])\n",
    "        bert_inp_tot.append((bert_inp['input_ids'][0], \n",
    "                             bert_inp['token_type_ids'][0], \n",
    "                             bert_inp['attention_mask'][0]))\n",
    "\n",
    "    input_ids = np.asarray(input_ids)\n",
    "    token_type_ids = np.asarray(token_type_ids)\n",
    "    attention_mask = np.array(attention_mask)\n",
    "    \n",
    "    return input_ids, token_type_ids, attention_mask, bert_inp_tot\n",
    "    \n",
    "\n",
    "# Fonction de création des features\n",
    "def feature_BERT_fct(model, model_type, sentences, max_length, b_size, mode='HF') :\n",
    "    batch_size = b_size\n",
    "    batch_size_pred = b_size\n",
    "    bert_tokenizer = transformers.AutoTokenizer.from_pretrained(model_type)\n",
    "    time1 = time.time()\n",
    "\n",
    "    for step in range(len(sentences)//batch_size) :\n",
    "        idx = step*batch_size\n",
    "        input_ids, token_type_ids, attention_mask, bert_inp_tot = bert_inp_fct(sentences[idx:idx+batch_size], \n",
    "                                                                      bert_tokenizer, max_length)\n",
    "        \n",
    "        if mode=='HF' :    # Bert HuggingFace\n",
    "            outputs = model.predict([input_ids, attention_mask, token_type_ids], batch_size=batch_size_pred)\n",
    "            last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        if mode=='TFhub' : # Bert Tensorflow Hub\n",
    "            text_preprocessed = {\"input_word_ids\" : input_ids, \n",
    "                                 \"input_mask\" : attention_mask, \n",
    "                                 \"input_type_ids\" : token_type_ids}\n",
    "            outputs = model(text_preprocessed)\n",
    "            last_hidden_states = outputs['sequence_output']\n",
    "             \n",
    "        if step ==0 :\n",
    "            last_hidden_states_tot = last_hidden_states\n",
    "            last_hidden_states_tot_0 = last_hidden_states\n",
    "        else :\n",
    "            last_hidden_states_tot = np.concatenate((last_hidden_states_tot,last_hidden_states))\n",
    "    \n",
    "    features_bert = np.array(last_hidden_states_tot).mean(axis=1)\n",
    "    \n",
    "    time2 = np.round(time.time() - time1,0)\n",
    "    print(\"temps traitement : \", time2)\n",
    "     \n",
    "    return features_bert, last_hidden_states_tot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e33c130",
   "metadata": {},
   "source": [
    "</Br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bbc64f",
   "metadata": {},
   "source": [
    "## BERT HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852f424f",
   "metadata": {},
   "source": [
    "### 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9d0385",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 64\n",
    "batch_size = 10\n",
    "model_type = 'bert-base-uncased'\n",
    "model = transformers.TFAutoModel.from_pretrained(model_type)\n",
    "sentences = df['sentence_dl'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c948161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création des features\n",
    "\n",
    "features_bert, last_hidden_states_tot = feature_BERT_fct(model, model_type, sentences, \n",
    "                                                         max_length, batch_size, mode='HF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eba2de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARI, X_tsne, labels = ARI_fct(features_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da2501f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TSNE_visu_fct(X_tsne, y_cat_num, labels, ARI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dbab38",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_cat_num, labels,class_labels=l_cat,normalize_f=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d660932",
   "metadata": {},
   "source": [
    "### 'cardiffnlp/twitter-roberta-base-sentiment'\n",
    "* Modèle pré-entraîné sur des tweets pour l'analyse de sentiment = particulièrement adapté au contexte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f44eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 64\n",
    "batch_size = 10\n",
    "model_type = 'cardiffnlp/twitter-roberta-base-sentiment'\n",
    "model = transformers.TFAutoModel.from_pretrained(model_type)\n",
    "sentences = df['sentence_dl'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967929dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_bert, last_hidden_states_tot = feature_BERT_fct(model, model_type, sentences, \n",
    "                                                         max_length, batch_size, mode='HF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612db667",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARI, X_tsne, labels = ARI_fct(features_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bfc235",
   "metadata": {},
   "outputs": [],
   "source": [
    "TSNE_visu_fct(X_tsne, y_cat_num, labels, ARI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11fed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_cat_num, labels,class_labels=l_cat,normalize_f=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d690eb4",
   "metadata": {},
   "source": [
    "</Br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4db172",
   "metadata": {},
   "source": [
    "#### <a name=\"C432\"><font color='green'>4.3.b USE - Universal Sentence Encoder </font></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d18ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# import tensorflow_hub as hub\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import metrics as kmetrics\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Bert\n",
    "import transformers\n",
    "#from transformers import *\n",
    "\n",
    "os.environ[\"TF_KERAS\"]='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cf4dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)\n",
    "print(tensorflow.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec57a58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8938efb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_USE_fct(sentences, b_size) :\n",
    "    batch_size = b_size\n",
    "    time1 = time.time()\n",
    "\n",
    "    for step in range(len(sentences)//batch_size) :\n",
    "        idx = step*batch_size\n",
    "        feat = embed(sentences[idx:idx+batch_size])\n",
    "\n",
    "        if step ==0 :\n",
    "            features = feat\n",
    "        else :\n",
    "            features = np.concatenate((features,feat))\n",
    "\n",
    "    time2 = np.round(time.time() - time1,0)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0b2303",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5206040a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "sentences = df['sentence_dl'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473022d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_USE = feature_USE_fct(sentences, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e939c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARI, X_tsne, labels = ARI_fct(features_USE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310d882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TSNE_visu_fct(X_tsne, y_cat_num, labels, ARI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7a01fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_cat_num, labels,class_labels=l_cat,normalize_f=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6daf3c",
   "metadata": {},
   "source": [
    "</Br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc06e02f",
   "metadata": {},
   "source": [
    "# <a name=\"C5\"><font color='pink'>**Partie 5 : Conclusion**</font></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dadaa0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb37d2f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217f5aa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9a0530",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "364bcf83",
   "metadata": {},
   "source": [
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6f4f1c",
   "metadata": {},
   "source": [
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5a44ed",
   "metadata": {},
   "source": [
    "</br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
